{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Introduction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNxTF7HyXez6qTQbvVISySq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sowmyarajesh/ML-NLP/blob/main/NLP_EmotionDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EEzfTMYJu3eU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset\n",
        "\n",
        "Get the original emotions data from https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp\n",
        "\n",
        "copy of it is provided in the git repo too https://github.com/sowmyarajesh/ML-NLP\n",
        "\n",
        "This dataset has a sentence and its associated emotion name seperated by ;.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sdg8m-48Vu92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainData = pd.read_csv('train.txt', sep=';', header=None)\n",
        "valData = pd.read_csv('val.txt', sep=';', header=None)\n",
        "testData = pd.read_csv('test.txt', sep=';', header=None)\n",
        "trainData.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrmbobDDcwe3",
        "outputId": "89368335-bf98-4a98-d915-4660dd0cfa7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Int64Index([0, 1], dtype='int64')"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainData[1].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1JR5aJLtZN8",
        "outputId": "ca8d4cd2-5082-4a49-f008-7ea1c598c1bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "joy         5362\n",
              "sadness     4666\n",
              "anger       2159\n",
              "fear        1937\n",
              "love        1304\n",
              "surprise     572\n",
              "Name: 1, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Preprocessing"
      ],
      "metadata": {
        "id": "kx7XwSigfElD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "padding_type='post'\n",
        "oov_label =\"<oov>\""
      ],
      "metadata": {
        "id": "CGSzqIavR7z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_sentences = trainData[0].values\n",
        "tr_labels = trainData[1].values\n",
        "\n",
        "val_sentences = valData[0].values\n",
        "val_labels = valData[1].values\n",
        "\n",
        "test_sentences = testData[0].values\n",
        "test_labels = testData[1].values"
      ],
      "metadata": {
        "id": "7yMjPDOSdfJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X\n",
        "tokenizer  = Tokenizer(oov_token=oov_label)\n",
        "tokenizer.fit_on_texts(tr_sentences)\n",
        "XIndex = tokenizer.word_index\n",
        "train_seq = tokenizer.texts_to_sequences(tr_sentences)\n",
        "train_X = np.array(pad_sequences(train_seq,padding=padding_type))\n",
        "print(\"creating X from sentences\")\n",
        "print(\"padded sequence[0] = \",train_X[0])\n",
        "print(\"padded sequence-> X.shape: \",train_X.shape)\n",
        "print(\"Length of input index: \",len(XIndex.keys()))\n",
        "\n",
        "#y\n",
        "tokenizerY  = Tokenizer(oov_token=oov_label)\n",
        "tokenizerY.fit_on_texts(tr_labels)\n",
        "yIndex = tokenizerY.word_index\n",
        "train_Labels = np.array([y[0] for y in tokenizerY.texts_to_sequences(tr_labels)])\n",
        "print(yIndex)\n",
        "print(train_Labels[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NF_5PdIbfSmA",
        "outputId": "a0f170ec-01eb-49ec-d2fa-52dfd7c3c033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  2 139   3 679   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "(16000, 66)\n",
            "15213\n",
            "{'<oov>': 1, 'joy': 2, 'sadness': 3, 'anger': 4, 'fear': 5, 'love': 6, 'surprise': 7}\n",
            "[3 3 4 6 4 3 7 5 2 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we have 16000 trainig sequences with 66 tokens. Now convert the text to sequence with the same training tokens for validation and test data"
      ],
      "metadata": {
        "id": "CCLeFZ8Hq4tP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_seq = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_padded = np.array(pad_sequences(test_seq,padding=padding_type))\n",
        "test_y = np.array([y[0] for y in tokenizerY.texts_to_sequences(test_labels)])\n",
        "print(test_padded[0])\n",
        "print(test_padded.shape)\n",
        "test_y[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMpllWR7qkYO",
        "outputId": "facc0093-52bc-457f-9eba-6bdad7c38af4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  17    8  203  715   15   17   26   46 5595  114   58    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0]\n",
            "(2000, 61)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 3, 3, 2, 3, 5, 4, 2, 2, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_seq = tokenizer.texts_to_sequences(val_sentences)\n",
        "val_padded = np.array(pad_sequences(val_seq,padding=padding_type))\n",
        "val_y = np.array([y[0] for y in tokenizerY.texts_to_sequences(val_labels)])\n",
        "print(val_padded[0])\n",
        "print(val_padded.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vs65AZyysNpc",
        "outputId": "36d8b061-472f-4bed-ef9d-19cca268105b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   17     8   157   260     4   343    16    51    19   212 11289    50\n",
            "    10    13   533     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0]\n",
            "(2000, 61)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training/ validation"
      ],
      "metadata": {
        "id": "YJbyLWhFzLUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(XIndex.keys())+1\n",
        "embedding_dim = len(yIndex.keys())+1\n",
        "max_length = tr_padded.shape[1]  # number of columns in the input matrix\n",
        "epochs = 30"
      ],
      "metadata": {
        "id": "vn6dfOsr4cTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(24, activation='relu'),\n",
        "    tf.keras.layers.Dense(8, activation='softmax')\n",
        "])\n",
        "model.compile( optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],)"
      ],
      "metadata": {
        "id": "Lo261ACksrI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(tr_padded, train_Labels, epochs=epochs, validation_data=(val_padded, val_y), verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "TyjmS_Hf2No2",
        "outputId": "40a70fa8-5bb4-4e81-9ae7-71930368af24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-e658cc765e5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1525, in test_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1514, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1507, in run_step  **\n        outputs = model.test_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1471, in test_step\n        y_pred = self(x, training=False)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_5\" is incompatible with the layer: expected shape=(None, 66), found shape=(None, 61)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZUoLHMUBRX5u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}