{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Introduction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOeFwvHQd5rQw0i+Yf2cUev",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sowmyarajesh/ML-NLP/blob/main/NLP_SarcasticHeadline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EEzfTMYJu3eU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sdg8m-48Vu92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization and sequencing\n",
        "\n",
        "First step is tokenize the training text data using Tokenizer() function. Then, convert the sentences to sequence of token. \n",
        "\n",
        "**parameters:**\n",
        "\n",
        "- num_words: maximum number of unique words by frequency. *Default => will consider all the unique words in the data.*\n",
        "\n",
        "- filters: a string where each element is a character that will be filtered from the texts. *Default => all punctuation, tabs and line breaks, except the ' character.*\n",
        "\n",
        "- lower: Whether to convert the texts to lowercase. *Default => true*\n",
        "\n",
        "- split: a string value that can be used as separator for word splitting. *Default => None*\n",
        "\n",
        "- char_level: should every character in the data be converted to token? *Default => False*\n",
        "\n",
        "- oov_token: string representing the out-of-vocabulary values. \n",
        "\n",
        "- analyzer: Custom analyzer function to split the text. *Default => text_to_word_sequence*\n",
        "\n",
        "\n",
        "**Methods:**\n",
        "\n",
        "- fit_on_sequences(sequences):  updates the internal voabulary list based on the sequences. [*Requires sequences_to_matrix() called before this function*]\n",
        "\n",
        "- fit_on_texts(data): updates the internal vocabulary list based on the words in the data.\n",
        "\n",
        "- texts_to_sequences(data): Transforms each text in texts to a sequence of integers.\n",
        "\n",
        "- texts_to_matrix(textArray, mode='binary'): Convert a list of texts to a Numpy matrix. [*Argument mode =>[\"binary\", \"count\", \"tfidf\", \"freq\"]* ]\n",
        "\n",
        "\n",
        "- get_config(): Returns the tokenizer configuration as Python dictionary.\n",
        "- to_json(): returns a JSON string containing the tokenizer configuration.\n",
        "- sequences_to_matrix(sequences, mode='binary'): Converts a list of sequences into a Numpy matrix. [*Argument mode =>[ \"binary\", \"count\", \"tfidf\", \"freq\"]* ]\n",
        "- sequences_to_texts(sequences): Converts a list of sequences into a text array.\n",
        "- sequences_to_texts_generator(sequences): Transforms each sequence in sequences to a list of texts(strings).\n"
      ],
      "metadata": {
        "id": "mwIOOdSSu4zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Sentences = [\n",
        "             'My name is Sam',\n",
        "             'My name is Bob',\n",
        "             'My Name is Bob',\n",
        "             'My name is sam!',\n",
        "             'I like to play baseball',\n",
        "             'I like go out for walking'\n",
        "]\n",
        "# tokenizer to get the first 100 most frequent words\n",
        "tokenizer = Tokenizer(num_words=100)\n",
        "tokenizer.fit_on_texts(Sentences)\n",
        "word_index=tokenizer.word_index\n",
        "print(word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y92gZekivFrH",
        "outputId": "4859a7fb-e27a-4d50-e609-cfa4b3696164"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'my': 1, 'name': 2, 'is': 3, 'sam': 4, 'bob': 5, 'i': 6, 'like': 7, 'to': 8, 'play': 9, 'baseball': 10, 'go': 11, 'out': 12, 'for': 13, 'walking': 14}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = tokenizer.texts_to_sequences(Sentences)\n",
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALguaxMNwHaD",
        "outputId": "d684dd93-419e-402b-de5b-cc1c80f475de"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 3, 4], [1, 2, 3, 5], [1, 2, 3, 5], [1, 2, 3, 4], [6, 7, 8, 9, 10], [6, 7, 11, 12, 13, 14]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the test data does not have tokened items. creating the sequence will miss in the valaues from the token. \n"
      ],
      "metadata": {
        "id": "NJXjKIc66e9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = ['Iam Bob', 'My name is Suman']\n",
        "tSeq = tokenizer.texts_to_sequences(test_sentence)\n",
        "print(tSeq)"
      ],
      "metadata": {
        "id": "4oDZKW2GvhCv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "846218dd-a3ea-4963-d282-8b98e757bae7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5], [1, 2, 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To avoid missing the words in the sequence, we can use the oov_token parameter in the tokenizer function. This will act as handler for out of vocabulary items. all the out of vocabulary items will be replaced with this value"
      ],
      "metadata": {
        "id": "uqvCcbg-7A3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer1 = Tokenizer(num_words=100,oov_token=\"<unknown>\")\n",
        "tokenizer1.fit_on_texts(Sentences)\n",
        "seq1 = tokenizer1.texts_to_sequences(Sentences)\n",
        "tSeq1 = tokenizer1.texts_to_sequences(test_sentence)\n",
        "print(tokenizer.word_index)\n",
        "print(seq1)\n",
        "print(tSeq1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RDQmdap69Mc",
        "outputId": "1ab29a89-be5c-4402-d0bf-1ddeeee18d82"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'my': 1, 'name': 2, 'is': 3, 'sam': 4, 'bob': 5, 'i': 6, 'like': 7, 'to': 8, 'play': 9, 'baseball': 10, 'go': 11, 'out': 12, 'for': 13, 'walking': 14}\n",
            "[[2, 3, 4, 5], [2, 3, 4, 6], [2, 3, 4, 6], [2, 3, 4, 5], [7, 8, 9, 10, 11], [7, 8, 12, 13, 14, 15]]\n",
            "[[1, 6], [2, 3, 4, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Padding:\n",
        "\n",
        "When the number of words in each sentences are of different length, it is hard to make proper 2D matrix of data like image files. \n",
        "\n",
        "To handle this situation, pad_sequence is used."
      ],
      "metadata": {
        "id": "8XK5NtWJ8QwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "padded_seq1 = pad_sequences(seq1,padding='post')\n",
        "print(padded_seq1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ju1nOrxD7wOA",
        "outputId": "1e79541a-e516-45bc-8c2b-a0718b932389"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2  3  4  5  0  0]\n",
            " [ 2  3  4  6  0  0]\n",
            " [ 2  3  4  6  0  0]\n",
            " [ 2  3  4  5  0  0]\n",
            " [ 7  8  9 10 11  0]\n",
            " [ 7  8 12 13 14 15]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_seq2 = pad_sequences(seq1) # defaults to 'pre'\n",
        "print(padded_seq2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrlQ5yru9Gdn",
        "outputId": "588213e2-b41c-483b-c4a8-e569bbb08009"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  0  2  3  4  5]\n",
            " [ 0  0  2  3  4  6]\n",
            " [ 0  0  2  3  4  6]\n",
            " [ 0  0  2  3  4  5]\n",
            " [ 0  7  8  9 10 11]\n",
            " [ 7  8 12 13 14 15]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_seq3 = pad_sequences(seq1,padding='pre')\n",
        "print(padded_seq3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pubEOon29LVO",
        "outputId": "ddf89ca1-3167-47eb-84c3-f9bcc93a483f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  0  2  3  4  5]\n",
            " [ 0  0  2  3  4  6]\n",
            " [ 0  0  2  3  4  6]\n",
            " [ 0  0  2  3  4  5]\n",
            " [ 0  7  8  9 10 11]\n",
            " [ 7  8 12 13 14 15]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "padding value is set to 0 for default. But it can be set to a float or string value using 'value' argument"
      ],
      "metadata": {
        "id": "Ufi8gt5R--RB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "padded_seq7 = pad_sequences(seq1,padding='post', value=-1)\n",
        "print(padded_seq7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKzZxU5j-6z0",
        "outputId": "ac85811f-ab31-405c-c5ad-964e1dd74143"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2  3  4  5 -1 -1]\n",
            " [ 2  3  4  6 -1 -1]\n",
            " [ 2  3  4  6 -1 -1]\n",
            " [ 2  3  4  5 -1 -1]\n",
            " [ 7  8  9 10 11 -1]\n",
            " [ 7  8 12 13 14 15]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the padding length is needed to be limited instead of maximum possible sentence length, we will set the maxlen argument of the pad_sequences function. This will truncate the length of sentences to maxlen of words."
      ],
      "metadata": {
        "id": "7JhaWB8x9iYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "padded_seq4 = pad_sequences(seq1,maxlen=4)\n",
        "print(padded_seq4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDIL3I2t9ejD",
        "outputId": "136a9666-c92a-4f78-80e0-41be08df423e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2  3  4  5]\n",
            " [ 2  3  4  6]\n",
            " [ 2  3  4  6]\n",
            " [ 2  3  4  5]\n",
            " [ 8  9 10 11]\n",
            " [12 13 14 15]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_seq5= pad_sequences(seq1,maxlen=4, padding='post')\n",
        "print(padded_seq5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4OiJPur9_LB",
        "outputId": "51ffd321-4132-43d9-c785-27f973ca3b6b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2  3  4  5]\n",
            " [ 2  3  4  6]\n",
            " [ 2  3  4  6]\n",
            " [ 2  3  4  5]\n",
            " [ 8  9 10 11]\n",
            " [12 13 14 15]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above method, the truncation happens only on the prefix irrespective of the padding status. To set the truncation side to posterior, we will have to set 'truncating' property. default is 'pre' "
      ],
      "metadata": {
        "id": "ihHj0tuh-dKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "padded_seq6 = pad_sequences(seq1,maxlen=5, padding='post', truncating='post')\n",
        "print(padded_seq6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeodjhAJ-Ut0",
        "outputId": "22172d79-0e08-4cdc-bdae-6da93a05c850"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2  3  4  5  0]\n",
            " [ 2  3  4  6  0]\n",
            " [ 2  3  4  6  0]\n",
            " [ 2  3  4  5  0]\n",
            " [ 7  8  9 10 11]\n",
            " [ 7  8 12 13 14]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**fit_on_sequences(sequences)**"
      ],
      "metadata": {
        "id": "UdNlTEiTF4YC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = [\n",
        "             ['My', 'name', 'is','Sam'],\n",
        "             ['My', 'name', 'is','Bob'],\n",
        "             ['I', 'like', 'to','play','baseball'],\n",
        "             ['I', 'like', 'icecream']\n",
        "]\n",
        "seq2 = tokenizer1.sequences_to_matrix(seq1, mode='binary')\n",
        "seq21 = tokenizer1.fit_on_sequences(seq2)\n",
        "print(seq2)\n",
        "print(seq21)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4XjGxBS_U-j",
        "outputId": "d1329972-871b-4477-da05-a9d2686dbb20"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]]\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5vLrJ5QTGHus"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}